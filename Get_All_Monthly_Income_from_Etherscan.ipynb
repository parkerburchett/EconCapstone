{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Get All Monthly Income from Etherscan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMKEVerqD1HmNSN71fXtTDK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parkerburchett/EconCapstone/blob/main/Get_All_Monthly_Income_from_Etherscan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-6c44YfUea1"
      },
      "source": [
        "# Constants\n",
        "parker_wallet = '0x76fb6d38f28c44a13380220df21363bd7af45ee1'\n",
        "ethan_wallet = '0xceb4d0ca821420cf2553b9e244f6b52364613f94'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H16oV8RENRNw"
      },
      "source": [
        "## This is the first draft of the notebook I am writing for my Economics Capstone to get a published paper. \\\n",
        "\n",
        "https://www.etherchain.org/charts Look at \"mining revenue\" gives USD value per day of a GH/s. Use this instead of building it yourself.\n",
        "### Outline: What will this notebook do?\n",
        "\n",
        "1. Use the Etherscan.io charts to convert into production \n",
        "* Block Number to Date.(Year, Month)\n",
        "* (Year, Month) to Monthly average Value of 1 day of mining at 1 GH/s (Etherscan.io chart). Just average by month. \n",
        "\n",
        "* (Year, Month) to Average ETH Price. \n",
        "\n",
        "\n",
        "* You will need a dictionary that looks like \n",
        "(Year, Month): Average Gh/s needed to mine make 1 ETH \n",
        "### Called production dictionary. \n",
        "\n",
        "2. You will need to write custom Estimate_hashrate \n",
        "\n",
        "*Pseudocode:\n",
        "  Estimate_HashRate(Year, Month, ETH earned this month):\n",
        "\n",
        "  estimated_hashrate_in_ghS = Production_Dictionary[(Year, Month) * ETH earned  this month)\n",
        "\n",
        "\n",
        "3. Use the Etherscan.io API to get all of the transactions from 5 large pools, and store that in pyspark on Google Colab. You will need to save that to someplace in the cloud so that you can get it later and put the public link. I think drop box is the right place for it.\n",
        "\n",
        " -> What I want the data to look like.\n",
        " This information is factual raw data. \n",
        "(Block Number of Transaction, Pool Address, Miner Address, Amount of ETH)\n",
        "\n",
        "\n",
        "Every Single Transaction will be mapped into this form. \n",
        "\n",
        "(Block Number, Day, Year, Month, Pool Address, Miner Address, Amount of ETH, Estimated_hashrate)\n",
        "\n",
        "This data will then be grouped by Miner_address and Month.\n",
        "\n",
        "Intermediate(after group by) result:\n",
        "\n",
        "Month, Year, Pool Address, Miner Address, ETH Earned this month, Estimated Hashrate, (ETH earned this month * Average Monthly Price of ETH) as monthly USD Revenue.\n",
        "\n",
        "Frame work for Miner Features\n",
        "This is how I am choosing to categorize Miners. \n",
        "\n",
        "## Miner Features\n",
        "Start month ( cast this as an int with January 2015 as 1). # month of first  income).\n",
        "End Month (cast this as an int with March 2021 as the 80 or whatever the number happens to be). Last month with income. or + infinity if that was march 2021. Age as number of full months,\n",
        "Median Hashrate, as size. \n",
        "\n",
        "\n",
        "\n",
        "Dummy Varibles for Pool address, \n",
        "Dummy Variables for if they went to a different\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Framework for OLS regression:\n",
        "\n",
        "##Independent variable: \n",
        "* Estimated Monthly Hashrate.\n",
        "\n",
        "###Dependent Variables\n",
        "Miner Features. \n",
        "Year\n",
        "Month,\n",
        "Average ETH price,\n",
        "average Gh/s value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eldsm3TLLL_l"
      },
      "source": [
        "### Setup Pyspark and Connect it to your google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoBD2liiLBZ0",
        "outputId": "c2917300-be6c-4e8d-bd85-7c3b002d3b1a"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Open Connection to Google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "print(\"\\n\\nDRIVE SETUP\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n",
            "openjdk-8-jdk-headless is already the newest version (8u282-b08-0ubuntu1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "\n",
            "DRIVE SETUP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D57YpSnyLnN1"
      },
      "source": [
        "## Install Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HRjFYf1LmL-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import math\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from functools import reduce\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import DataType\n",
        "from pyspark import SparkContext, SparkConf\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK8odYhULw-2"
      },
      "source": [
        "### Create Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1clEsoKVLwbN",
        "outputId": "55a0ccab-e3f6-460d-941a-abd42b5313ce"
      },
      "source": [
        "# create the spark session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\") # copy and pasted this, dont' know what it means\n",
        "# create the context\n",
        "# try catch block lets you start again from the top without causing problems. \n",
        "try:\n",
        "  sc.stop()\n",
        "  sc = pyspark.SparkContext(conf=conf)\n",
        "except:\n",
        "  sc = pyspark.SparkContext(conf=conf)\n",
        "  \n",
        "spark = SparkSession.builder \\\n",
        "    .master('local[*]') \\\n",
        "    .config(\"spark.driver.memory\", \"15g\") \\\n",
        "    .appName('my-cool-app') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# you are just copy and pasting the spark config details\n",
        "\n",
        "print('Successfully Created Spark Session')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully Created Spark Session\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gInzfyainX8H"
      },
      "source": [
        "## Methods to interact with the Etherscan.io API\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7x-vmI5NPLL"
      },
      "source": [
        "def read_api_key():\n",
        "  api_key_json = open(r'/content/drive/MyDrive/etherScan_apiKey.json') # read in my private Etherscan.io API key from my Google Drive\n",
        "  etherscan_api_key =json.load(api_key_json)['key']\n",
        "  api_key_json.close()\n",
        "  return etherscan_api_key\n",
        "\n",
        "ETHERSCAN_API_KEY = read_api_key() # You api key is a constant so the variable name is UPPER CASE\n",
        "ETHERMINE_WALLET = '0xea674fdde714fd979de3edf0f56aa9716b898ec8'\n",
        "ETHERMINE_WALLET\n",
        "def query_normal_transactions(wallet_address, startblock=0, endblock=99999999): \n",
        "  \"\"\"\n",
        "    Ping Etherscan.io and get the most recent 10k transactions for this wallet.\n",
        "    Each of these takes ~4 seconds\n",
        "  \"\"\"\n",
        "  api_request_text =f'https://api.etherscan.io/api?module=account&action=txlist&address={wallet_address}&startblock={startblock}&endblock={endblock}&sort=asc&apikey={ETHERSCAN_API_KEY}'\n",
        "     \n",
        "  response = requests.get(api_request_text) # you cannot make more than 5 requests per second. This has yet to be a problem\n",
        "  api_response_text = json.loads(response.text) \n",
        "  \n",
        "  # error handling to make sure that the call is going through\n",
        "  if api_response_text['message'] != 'OK': # untested\n",
        "    print(f'you got an error at address={wallet_address} \\nstartblock= {startblock}\\nendblock={endblock}')\n",
        "\n",
        "  else:\n",
        "    transaction_list = api_response_text['result']\n",
        "    simplified_transactions = parse_normal_transactions(transaction_list=transaction_list, wallet_address=wallet_address)\n",
        "\n",
        "    #exclude the last N transactions since the might overlapp\n",
        "    largest_block = int(simplified_transactions[-1][2]) # O(1)\n",
        "    while simplified_transactions[-1][2] == largest_block:\n",
        "      simplified_transactions.pop(-1) # O(1) * 100 approx upper bound. This gets rid of all the blocks that are the same as the last block\n",
        "\n",
        "    column_names = ['to_address',\n",
        "                    'from_address',\n",
        "                    'block_number',\n",
        "                    'transaction_year',\n",
        "                    'transaction_month',\n",
        "                    'value_in_ether']\n",
        "    \n",
        "    # create a rdd of this batch of transactions\n",
        "    rdd = spark.createDataFrame(simplified_transactions, schema=column_names)\n",
        "    return rdd, largest_block\n",
        "\n",
        "def parse_normal_transactions(transaction_list: list, wallet_address: str)-> list:\n",
        "    \"\"\"\n",
        "      Converts transaction data into a easy to read from and excludes the last block\n",
        "    :param transaction_list: A list of dictionary objects. Each dictionary is a single transaction.\n",
        "    :return:\n",
        "        simple_transactions: a list of tuples storing (to_address, from_address, Block number, Year, Month, Value in Ether)\n",
        "                                      Data types are: (String, String, int, int, int, float)\n",
        "    \"\"\"\n",
        "    simplified_transactions =[(trans_dict['to'],\n",
        "                              trans_dict['from'],\n",
        "                              int(trans_dict['blockNumber']),\n",
        "                              datetime.datetime.fromtimestamp(int(trans_dict['timeStamp'])).year,\n",
        "                               datetime.datetime.fromtimestamp(int(trans_dict['timeStamp'])).month,\n",
        "                              int(trans_dict['value'])/math.pow(10, 18)) # convert the 'value' column from wei to Ether\n",
        "                              for trans_dict in transaction_list if trans_dict['from'] == wallet_address\n",
        "                              ]\n",
        "    return simplified_transactions"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEPpIh68GY-h"
      },
      "source": [
        "## Next steps are to get a dictionary of block: time chunks. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYZqQ6IUlIMG",
        "outputId": "3e499411-3765-4bad-8eff-b26f71087b3a"
      },
      "source": [
        "start_year = 2016\n",
        "years = [datetime.datetime(start_year+i,1,1) for i in range(0,6)]\n",
        "unix_timestamp_years = [int(year.timestamp()) for year in years]\n",
        "unix_timestamp_march31_2016 = int(datetime.datetime(2021,3,31).timestamp())\n",
        "\n",
        "command_for_block_after_timestamp = f'https://api.etherscan.io/api?module=block&action=getblocknobytime&timestamp={unix_timestamp_years[0]}&closest=after&apikey={ETHERSCAN_API_KEY}'\n",
        "\n",
        "year_block_dict ={}\n",
        "\n",
        "for unix_time in unix_timestamp_years:\n",
        "  time.sleep(.2) # don't overwhelm the API\n",
        "  response = requests.get(f'https://api.etherscan.io/api?module=block&action=getblocknobytime&timestamp={unix_time}&closest=after&apikey={ETHERSCAN_API_KEY}')\n",
        "  response_text = json.loads(response.text) \n",
        "  block = response_text['result']\n",
        "  year_block_dict[int(datetime.datetime.fromtimestamp(unix_time).year)]= block\n",
        "\n",
        "print(year_block_dict)\n",
        "response = requests.get(f'https://api.etherscan.io/api?module=block&action=getblocknobytime&timestamp={unix_timestamp_march31_2016}&closest=after&apikey={ETHERSCAN_API_KEY}')\n",
        "response_text = json.loads(response.text) \n",
        "march_2021_block = response_text['result']\n",
        "march_2021_block\n",
        "year_block_dict['2022'] = march_2021_block # this is to make the code work. as written "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{2016: '778483', 2017: '2912407', 2018: '4832686', 2019: '6988615', 2020: '9193266', 2021: '11565019'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd57cH8m1g6z"
      },
      "source": [
        "def get_monthly_miner_income_statements_chunk(year,pool_address):\n",
        "  \"\"\"\n",
        "    Ping every transaction where {pool_address} is the sender and cast it as a pyspark dataframe.\n",
        "\n",
        "    add a year_month column. \n",
        "\n",
        "    Group by to_address and year_month\n",
        "\n",
        "    cast as a pandas df\n",
        "\n",
        "    Save the document to a file in this local runtime of google drive. \n",
        "  \"\"\"\n",
        "  start = datetime.datetime.now()\n",
        "  min_block = int(year_block_dict[year]) # if year = 2018 then this gets all miner income in 2018\n",
        "  max_block = int(year_block_dict[year+1]) # all blocks must be within this range\n",
        "\n",
        "\n",
        "  i=0\n",
        "  #Inital call to set up transaction rdd. You will union transactions on to this.\n",
        "  transactions_rdd, largest_block = query_normal_transactions(wallet_address=pool_address, startblock=min_block, endblock=max_block)\n",
        "\n",
        "  while largest_block < max_block:\n",
        "    try:\n",
        "      # repeat the call picking up where you left off last call since you can only get 10,000 records per call\n",
        "      next_chunk_rdd, largest_block = query_normal_transactions(wallet_address=pool_address, startblock=largest_block) \n",
        "      transactions_rdd = transactions_rdd.union(next_chunk_rdd) # union removes duplicates\n",
        "      # this lets you see progress\n",
        "      if i %10 ==0:\n",
        "        time_dif = datetime.datetime.now() - start\n",
        "        start = datetime.datetime.now()\n",
        "        print(f'Call:{i} Time:{time_dif} Block:{largest_block} Blocks Left:{max_block - largest_block}')\n",
        "      i+=1\n",
        "    except:\n",
        "      print(f'you got an error at {largest_block}\\n {next_chunk_rdd.head(1)} ')\n",
        "\n",
        "  \n",
        "  # add the year_month column. \n",
        "  transactions_rdd = transactions_rdd.withColumn('year_month', concat(transactions_rdd.transaction_year,lit(\"-\"),\n",
        "                                                                      transactions_rdd.transaction_month))\n",
        "  global miner_df\n",
        "  miner_df = group_miners(transactions_rdd,pool_address, year)\n",
        "  save_miner_df_to_drive(miner_df, year, pool_address)\n",
        "\n",
        "\n",
        "\n",
        "def group_miners(transactions_rdd, pool_address, year):\n",
        "  \"\"\"\n",
        "\n",
        "    Group the transaction rdd into year_month, to address groups by summing the value_in_ether. \n",
        "  input: the transaction rdd for a single year.\n",
        "\n",
        "  Do this of this sql command\n",
        "\n",
        "     SELECT to_address, year_month, ROUND(sum(value_in_ether),9) as eth_earned\n",
        "                                    FROM raw_transactions \n",
        "                                    Where block_number < {max_block} and block_number > {min_block}\n",
        "                                    GROUP BY to_address, year_month\n",
        "                                    ORDER BY to_address, year_month  \n",
        "  \"\"\"\n",
        "\n",
        "  miner_rdd = transactions_rdd.groupby('to_address','year_month').sum('value_in_ether') \n",
        "  miner_rdd = miner_rdd.withColumn('from_address', lit(pool_address)) # add the pool address as a column might be unneccessary since you have this in the header\n",
        "  miner_rdd = miner_rdd.filter(miner_rdd['year_month'].contains(str(year))) # only include the  from the year you are considering\n",
        "  print('You have done data aggregation')\n",
        "  df = miner_rdd.toPandas()\n",
        "\n",
        "  # you might want to drop the other columns\n",
        "  # you would drop (Year, Month, and pool address column since it is in the header.\n",
        "  print('Verify that these months are correct')\n",
        "  print(df['year_month'].unique())\n",
        "  return df\n",
        "\n",
        "def save_miner_df_to_drive(miner_df, year, pool_address):\n",
        "  \"\"\"\n",
        "    Save the miner_df to your google drive in a particular folder\n",
        "    df: a grouped miner income statement\n",
        "    yera: the year you are saving data for. \n",
        "    pool_address: the ETH wallet address of the pool\n",
        "  \"\"\"\n",
        "  PATH_TO_SAVE = f'/content/drive/MyDrive/transaction_data/miner_income_{year}_{pool_address}.csv'\n",
        "  miner_df.to_csv(PATH_TO_SAVE,index=False) # save to your google drive\n",
        "  print('Saved to google drive')\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFY1_CDG9e5z"
      },
      "source": [
        "HIVEON_POOL= '0x1aD91ee08f21bE3dE0BA2ba6918E714dA6B45836'\n",
        "SPARK_POOL = '0x5A0b54D5dc17e0AadC383d2db43B0a0D3E029c4c'\n",
        "F2_POOL = '0x829BD824B016326A401d083B33D092293333A830'\n",
        "ZHIZHU_TOP_POOL = '0x04668Ec2f57cC15c381b461B9fEDaB5D451c8F7F'\n",
        "ETHERMINE_POOL = '0xEA674fdDe714fd979de3EdF0F56AA9716B898ec8'\n",
        "\n",
        "POOLS = [ETHERMINE_POOL, HIVEON_POOL, SPARK_POOL, F2_POOL, ZHIZHU_TOP_POOL]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "RD5W5Pnq8Gtm",
        "outputId": "e6cefdaf-648d-42d3-b550-57586a4e6ba3"
      },
      "source": [
        "%%time\n",
        "years = [2016,2017,2018,2019,2020,2021]\n",
        "for year in years:\n",
        "  for pool in POOLS:\n",
        "    get_monthly_miner_income_statements_chunk(year,pool)\n",
        "\n",
        "    #I think this should just work now when I run it from the top"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6e8fabd7db16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'years = [2016,2017,2018,2019,2020,2021]\\nfor y in years:\\n  get_monthly_miner_income_statements_chunk(2015,ETHERMINE_POOL)\\n\\n  #You get no months with this call:\\n# on year =2016 you get approx 100 calls in 5 minutes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-758ef4004117>\u001b[0m in \u001b[0;36mget_monthly_miner_income_statements_chunk\u001b[0;34m(year, pool_address)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mSave\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mlocal\u001b[0m \u001b[0mruntime\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgoogle\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \"\"\"\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mmin_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_block_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# if year = 2018 then this gets all miner income in 2018\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mmax_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_block_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# all blocks must be within this range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0httE21z3hd"
      },
      "source": [
        "# miner_rdd = local_transaction_rdd.groupby('to_address','year_month').sum('value_in_ether') \n",
        "# miner_rdd.toPandas()['year_month'].unique()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDVXl7jB5kCd"
      },
      "source": [
        "# miner_rdd = local_transaction_rdd.groupby('to_address','year_month').sum('value_in_ether') \n",
        "# miner_rdd = miner_rdd.withColumn('from_address', lit(pool_address)) # add the pool address as a column might be unneccessary since you have this in the header\n",
        "# miner_rdd = miner_rdd.filter(miner_rdd['year_month'].contains(str(year))) # only include the data where the month_year.conaints '2016\n",
        "# df = miner_rdd.toPandas()\n",
        "# # you might want to drop the other columns\n",
        "# # you would drop (Year, Month, and )\n",
        "# print('Verify that these months are correct')\n",
        "# print(df['year_month'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReZSkiycoGoE"
      },
      "source": [
        "### Estimated Timecosts \n",
        "\n",
        "This is the number of API calls you need to make for Ethermine.io \n",
        "Spark pool is similar in size, the other pools are much smaller. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcSH0GqqPo5X"
      },
      "source": [
        "# How many API calls do you need to make?\n",
        "n_transactions = 40000000\n",
        "reccords_per_call = 10000\n",
        "total_api_calls = n_transactions/reccords_per_call\n",
        "base_time_cost_per_api_call = 5 #Loose Uppperbound It is really 3.5 seconds\n",
        "seconds_needed = (base_time_cost_per_api_call *total_api_calls) \n",
        "hours = seconds_needed/3600 # 3600 is seconds in an hour\n",
        "print(f'Upper bound for API calls for ethermine {total_api_calls_needed}') # upper bound is 5 hours.\n",
        "print(f'Upper bound for hours to get all of Ethermine hours to get everything from ethermine {hours}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xIwf1A86gCq"
      },
      "source": [
        "\n",
        "\n",
        "# def group_miners1(transactions_rdd, max_block, min_block):\n",
        "#   \"\"\"\n",
        "#   Old version works at small scale breaks  as you hit year 3  due to memeory leak somewhere. \n",
        "#   If you restart the run every year it ought to work\n",
        "#   \"\"\"\n",
        "#   transactions_rdd.createTempTable(\"raw_transactions\") \n",
        "\n",
        "#   monthly_miner_revenue_query =f\"\"\" SELECT to_address, year_month, ROUND(sum(value_in_ether),9) as eth_earned\n",
        "#                                     FROM raw_transactions \n",
        "#                                     Where block_number < {max_block} and block_number > {min_block}\n",
        "#                                     GROUP BY to_address, year_month\n",
        "#                                     ORDER BY to_address, year_month \n",
        "#                                 \"\"\"\n",
        "\n",
        "#   miners_sql_df = spark.sql(monthly_miner_revenue_query)\n",
        "#   miners_sql_df = miners_sql_df.withColumn('from_address', lit(pool_address)) # add the pool address back into the rdd\n",
        "#   df = miners_sql_df.toPandas()\n",
        "#   print('Verify that these months are correct')\n",
        "#   print(df['year_month'].unique())\n",
        "#   return df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}