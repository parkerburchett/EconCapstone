{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook to Gather Ethermine Transaction data and do Cleaning and regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNwIa4ZjSIik4hQPZwqhnK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parkerburchett/EconCapstone/blob/main/Notebook_to_Gather_Ethermine_Transaction_data_and_do_Cleaning_and_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-6c44YfUea1"
      },
      "source": [
        "# Constants\n",
        "parker_wallet = '0x76fb6d38f28c44a13380220df21363bd7af45ee1'\n",
        "ethan_wallet = '0xceb4d0ca821420cf2553b9e244f6b52364613f94'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H16oV8RENRNw"
      },
      "source": [
        "## This is the first draft of the notebook I am writing for my Economics Capstone to get a published paper. \\\n",
        "\n",
        "https://www.etherchain.org/charts Look at \"mining revenue\" gives USD value per day of a GH/s. Use this instead of building it yourself.\n",
        "### Outline: What will this notebook do?\n",
        "\n",
        "1. Use the Etherscan.io charts to convert into production \n",
        "* Block Number to Date.(Year, Month)\n",
        "* (Year, Month) to Monthly average Value of 1 day of mining at 1 GH/s (Etherscan.io chart). Just average by month. \n",
        "\n",
        "* (Year, Month) to Average ETH Price. \n",
        "\n",
        "\n",
        "* You will need a dictionary that looks like \n",
        "(Year, Month): Average Gh/s needed to mine make 1 ETH \n",
        "### Called production dictionary. \n",
        "\n",
        "2. You will need to write custom Estimate_hashrate \n",
        "\n",
        "*Pseudocode:\n",
        "  Estimate_HashRate(Year, Month, ETH earned this month):\n",
        "\n",
        "  estimated_hashrate_in_ghS = Production_Dictionary[(Year, Month) * ETH earned  this month)\n",
        "\n",
        "\n",
        "3. Use the Etherscan.io API to get all of the transactions from 5 large pools, and store that in pyspark on Google Colab. You will need to save that to someplace in the cloud so that you can get it later and put the public link. I think drop box is the right place for it.\n",
        "\n",
        " -> What I want the data to look like.\n",
        " This information is factual raw data. \n",
        "(Block Number of Transaction, Pool Address, Miner Address, Amount of ETH)\n",
        "\n",
        "\n",
        "Every Single Transaction will be mapped into this form. \n",
        "\n",
        "(Block Number, Day, Year, Month, Pool Address, Miner Address, Amount of ETH, Estimated_hashrate)\n",
        "\n",
        "This data will then be grouped by Miner_address and Month.\n",
        "\n",
        "Intermediate(after group by) result:\n",
        "\n",
        "Month, Year, Pool Address, Miner Address, ETH Earned this month, Estimated Hashrate, (ETH earned this month * Average Monthly Price of ETH) as monthly USD Revenue.\n",
        "\n",
        "Frame work for Miner Features\n",
        "This is how I am choosing to categorize Miners. \n",
        "\n",
        "## Miner Features\n",
        "Start month ( cast this as an int with January 2015 as 1). # month of first  income).\n",
        "End Month (cast this as an int with March 2021 as the 80 or whatever the number happens to be). Last month with income. or + infinity if that was march 2021. Age as number of full months,\n",
        "Median Hashrate, as size. \n",
        "\n",
        "\n",
        "\n",
        "Dummy Varibles for Pool address, \n",
        "Dummy Variables for if they went to a different\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Framework for OLS regression:\n",
        "\n",
        "##Independent variable: \n",
        "* Estimated Monthly Hashrate.\n",
        "\n",
        "###Dependent Variables\n",
        "Miner Features. \n",
        "Year\n",
        "Month,\n",
        "Average ETH price,\n",
        "average Gh/s value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eldsm3TLLL_l"
      },
      "source": [
        "### Setup Pyspark and Connect it to your google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoBD2liiLBZ0",
        "outputId": "ea72c043-272f-44e5-b139-6b0f2f1ee5db"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Open Connection to Google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "print(\"\\n\\n DRIVE SETUP\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 49kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 44.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=cd02ab4da90dd6892ccd71e5ed891e66bda382d87e65507b6084fd50967eb148\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 160980 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u282-b08-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u282-b08-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u282-b08-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u282-b08-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u282-b08-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u282-b08-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Mounted at /content/drive\n",
            "\n",
            "\n",
            " DRIVE SETUP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D57YpSnyLnN1"
      },
      "source": [
        "## Install Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HRjFYf1LmL-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import math\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from functools import reduce\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import DataType\n",
        "from pyspark import SparkContext, SparkConf\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK8odYhULw-2"
      },
      "source": [
        "### Create Spark Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gInzfyainX8H"
      },
      "source": [
        "## Establish constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7x-vmI5NPLL"
      },
      "source": [
        "def read_api_key():\n",
        "  api_key_json = open(r'/content/drive/MyDrive/etherScan_apiKey.json') # read in my private Etherscan.io API key from my Google Drive\n",
        "  etherscan_api_key =json.load(api_key_json)['key']\n",
        "  api_key_json.close()\n",
        "  return etherscan_api_key\n",
        "\n",
        "ETHERSCAN_API_KEY = read_api_key() # You api key is a constant so the variable name is UPPER CASE\n",
        "ethermine_wallet = '0xea674fdde714fd979de3edf0f56aa9716b898ec8'\n",
        "def query_normal_transactions(wallet_address, startblock=0, endblock=99999999): \n",
        "  \"\"\"\n",
        "    Ping Etherscan.io and get the most recent 10k transactions for this wallet.\n",
        "    Each of these takes ~4 seconds\n",
        "  \"\"\"\n",
        "  api_request_text =f'https://api.etherscan.io/api?module=account&action=txlist&address={wallet_address}&startblock={startblock}&endblock={endblock}&sort=asc&apikey={ETHERSCAN_API_KEY}'\n",
        "     \n",
        "  response = requests.get(api_request_text) # Need to throttle this to 5 calls a second\n",
        "  api_response_text = json.loads(response.text) \n",
        "  # error handling to make sure that the call is going through\n",
        "  if api_response_text['message'] != 'OK': # untested\n",
        "    print(f'you got an error at address={wallet_address} \\nstartblock= {startblock}\\nendblock={endblock}')\n",
        "\n",
        "  else:\n",
        "    transaction_list = api_response_text['result']\n",
        "    simplified_transactions = parse_normal_transactions(transaction_list=transaction_list, wallet_address=wallet_address)\n",
        "\n",
        "    #exclude the last N transactions since the might overlapp\n",
        "    largest_block = simplified_transactions[-1][2] # O(1)\n",
        "    while simplified_transactions[-1][2] == largest_block:\n",
        "      simplified_transactions.pop(-1) # O(1) * 100 upper bound.\n",
        "\n",
        "    column_names = ['to_address',\n",
        "                    'from_address',\n",
        "                    'block_number',\n",
        "                    'transaction_year',\n",
        "                    'transaction_month',\n",
        "                    'value_in_ether']\n",
        "    \n",
        "    # create a rdd of this batch of transactions\n",
        "    rdd = spark.createDataFrame(simplified_transactions, schema=column_names)\n",
        "    return rdd, largest_block\n",
        "\n",
        "def parse_normal_transactions(transaction_list: list, wallet_address):\n",
        "    \"\"\"\n",
        "      Converts transaction data into a easy to read from and excludes the last block\n",
        "    :param transaction_list: A list of dictionary objects. Each dictionary is a single transaction.\n",
        "    :return:\n",
        "        simple_transactions: a list of tuples storing (to_address, from_address, Block number, Value in Ether)\n",
        "                                      Data types are: (String, String, int, year,, float)\n",
        "    \"\"\"\n",
        "    simplified_transactions =[(trans_dict['to'],\n",
        "                              trans_dict['from'],\n",
        "                              int(trans_dict['blockNumber']),\n",
        "                              datetime.datetime.fromtimestamp(int(trans_dict['timeStamp'])).year,\n",
        "                               datetime.datetime.fromtimestamp(int(trans_dict['timeStamp'])).month,\n",
        "                              int(trans_dict['value'])/math.pow(10, 18))\n",
        "                              for trans_dict in transaction_list if trans_dict['from'] == wallet_address\n",
        "                              ]\n",
        "    return simplified_transactions"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEPpIh68GY-h"
      },
      "source": [
        "## Next steps are to get a dictionary of block: time chunks. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "VYZqQ6IUlIMG",
        "outputId": "79302007-5cf9-436f-cbdc-4a8a08297d51"
      },
      "source": [
        "import datetime\n",
        "import requests\n",
        "\n",
        "start_year = 2016\n",
        "\n",
        "years = [datetime.datetime(start_year+i,1,1) for i in range(0,6)]\n",
        "\n",
        "unix_timestamp_years = [int(year.timestamp()) for year in years]\n",
        "\n",
        "unix_timestamp_march31_2016 = int(datetime.datetime(2021,3,31).timestamp())\n",
        "\n",
        "\n",
        "command_for_block_after_timestamp = f'https://api.etherscan.io/api?module=block&action=getblocknobytime&timestamp={unix_timestamp_years[0]}&closest=after&apikey={ETHERSCAN_API_KEY}'\n",
        "\n",
        "\n",
        "\n",
        "year_block_dict ={}\n",
        "\n",
        "for unix_time in unix_timestamp_years:\n",
        "  time.sleep(.2) # don't overwhelm the API\n",
        "  response = requests.get(f'https://api.etherscan.io/api?module=block&action=getblocknobytime&timestamp={unix_time}&closest=after&apikey={ETHERSCAN_API_KEY}')\n",
        "  response_text = json.loads(response.text) \n",
        "  block = response_text['result']\n",
        "  year_block_dict[datetime.datetime.fromtimestamp(unix_time).year]= block\n",
        "\n",
        "\n",
        "print(year_block_dict)\n",
        "\n",
        "\n",
        "response = requests.get(f'https://api.etherscan.io/api?module=block&action=getblocknobytime&timestamp={unix_timestamp_march31_2016}&closest=after&apikey={ETHERSCAN_API_KEY}')\n",
        "response_text = json.loads(response.text) \n",
        "march_2021_block = response_text['result']\n",
        "march_2021_block\n",
        "\n",
        "\n",
        "\n",
        "You now have all the blocks to properly batch the data\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{2016: '778483', 2017: '2912407', 2018: '4832686', 2019: '6988615', 2020: '9193266', 2021: '11565019'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'12143793'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heO_SZu8s9YV"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rappZLngtAU9",
        "outputId": "1485e49f-7a46-456a-bf94-ab33477160f9"
      },
      "source": [
        "response = requests.get(f'https://api.etherscan.io/api?module=block&action=getblocknobytime&timestamp={unix_timestamp_years[0]}&closest=after&apikey={ETHERSCAN_API_KEY}')\n",
        "response_text = json.loads(response.text) \n",
        "block =response_text['result']\n",
        "block"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'778483'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1clEsoKVLwbN"
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\") # copy and pasted this, dont' know what it means\n",
        "# create the context\n",
        "# try catch block lets you start again from the top without causing problems. \n",
        "try:\n",
        "  sc.stop()\n",
        "  sc = pyspark.SparkContext(conf=conf)\n",
        "except:\n",
        "  sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "print('Successfully Created Spark Session')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgBL6_0bKH4H"
      },
      "source": [
        "## Methods to get transactions from Etherscan.io API\n",
        "\n",
        "\n",
        "### You will want some try catch logic to make sure that you are not needlessly pinging Etherscan.io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxopgj-ejj_2"
      },
      "source": [
        "### Figure out what is the first and last block in each year. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiiWPgqHj9rY"
      },
      "source": [
        "# you want ot use this unit commant\n",
        "\n",
        "import datetime\n",
        "\n",
        "get block numer by time stamp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "unix_time = datetime "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ocR4dyogtJ1"
      },
      "source": [
        "# %%time\n",
        "# start = datetime.datetime.now()\n",
        "# ethermine_transaction_rdd, largest_block = query_normal_transactions(wallet_address=ethermine_wallet)\n",
        "# for i in range(4000): # the large call should be 4,000 ish. Just let it run while you are making dinner. see what happens.  \n",
        "#   try:\n",
        "#     next_chunk_rdd, largest_block = query_normal_transactions(wallet_address=ethermine_wallet, startblock=largest_block)\n",
        "#     ethermine_transaction_rdd = ethermine_transaction_rdd.union(next_chunk_rdd)\n",
        "#     if largest_block >= 12143978: # this the last block you want to consider\n",
        "#       break\n",
        "#     time_dif = datetime.datetime.now() -start\n",
        "#     start = datetime.datetime.now() \n",
        "#     print(f'Call:{i+1} Time:{time_dif}')\n",
        "#   except:\n",
        "#     print(f'you got an error at {largest_block}\\n {next_chunk_rdd.head(1)} ')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZh48kDxZNUr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFj5VQVcbkKe"
      },
      "source": [
        "### Only save the group by month_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDla_iVrb7X2"
      },
      "source": [
        "# rdd = ethermine_transaction_rdd\n",
        "# rdd = rdd.withColumn('year_month', concat(rdd.transaction_year,lit(\"-\"),rdd.transaction_month)) # add the year_month column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF3Oy0hIgbQ_"
      },
      "source": [
        "# rdd.registerTempTable(\"ethermine_transactions\")\n",
        "\n",
        "# monthly_miner_revenue_query =\"\"\"SELECT to_address, year_month, ROUND(sum(value_in_ether),9) as eth_earned\n",
        "# FROM ethermine_transactions \n",
        "# GROUP BY to_address, year_month\n",
        "# ORDER BY to_address, year_month \n",
        "# \"\"\"\n",
        "\n",
        "# miners = spark.sql(monthly_miner_revenue_query)\n",
        "# num_monthly_income_statements = miners.count()\n",
        "# print(f'You have {num_monthly_income_statements} income statements')\n",
        "# miners = miners.withColumn('from_address', lit(ethermine_wallet))\n",
        "# miners.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n85_oocFjsx-"
      },
      "source": [
        "## Save miner_grouped_rdd to a .csv file on your google drive. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Oq2b21jrfo"
      },
      "source": [
        "# miners_df = miners.toPandas()\n",
        "# miners_df.to_csv(r'/content/drive/MyDrive/miners.csv', index=False) # unclear what happens when there is alreadya file with stuff here.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAuBniUDvVEB"
      },
      "source": [
        "# with open(r'/content/drive/MyDrive/miners.csv', 'r') as data:\n",
        "#   num_lines_in_file = len(data.readlines())\n",
        "#   print(f'In your google drive you saved a file with: {num_lines_in_file} lines')\n",
        "#   print(f'There should be:                            {num_monthly_income_statements+1} lines ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReZSkiycoGoE"
      },
      "source": [
        "### Estimated Timecosts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcSH0GqqPo5X"
      },
      "source": [
        "# # How many API calls do you need to make?\n",
        "# n_transactions = 40000000\n",
        "# reccords_per_call = 10000\n",
        "# total_api_calls = n_transactions/reccords_per_call\n",
        "# base_time_cost_per_api_call = 5 #Loose Uppperbound\n",
        "# seconds_needed = (base_time_cost_per_api_call *total_api_calls) \n",
        "# hours = seconds_needed/3600\n",
        "# print(f'Upper bound for API calls for ethermine {total_api_calls_needed}') # upper bound is 5 hours.\n",
        "# print(f'Upper bound for hours to get all of Ethermine hours to get everything from ethermine {hours}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}