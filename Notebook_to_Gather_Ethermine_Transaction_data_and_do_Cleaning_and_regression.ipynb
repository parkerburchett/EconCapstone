{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook to Gather Ethermine Transaction data and do Cleaning and regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+qoq3L7Qpgpsfdz5OoTqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parkerburchett/EconCapstone/blob/main/Notebook_to_Gather_Ethermine_Transaction_data_and_do_Cleaning_and_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-6c44YfUea1"
      },
      "source": [
        "# Constants\n",
        "parker_wallet = '0x76fb6d38f28c44a13380220df21363bd7af45ee1'\n",
        "ethan_wallet = '0xceb4d0ca821420cf2553b9e244f6b52364613f94'"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H16oV8RENRNw"
      },
      "source": [
        "## This is the first draft of the notebook I am writing for my Economics Capstone to get a published paper. \\\n",
        "\n",
        "https://www.etherchain.org/charts Look at \"mining revenue\" gives USD value per day of a GH/s. Use this instead of building it yourself.\n",
        "### Outline: What will this notebook do?\n",
        "\n",
        "1. Use the Etherscan.io charts to convert into production \n",
        "* Block Number to Date.(Year, Month)\n",
        "* (Year, Month) to Monthly average Value of 1 day of mining at 1 GH/s (Etherscan.io chart). Just average by month. \n",
        "\n",
        "* (Year, Month) to Average ETH Price. \n",
        "\n",
        "\n",
        "* You will need a dictionary that looks like \n",
        "(Year, Month): Average Gh/s needed to mine make 1 ETH \n",
        "### Called production dictionary. \n",
        "\n",
        "2. You will need to write custom Estimate_hashrate \n",
        "\n",
        "*Pseudocode:\n",
        "  Estimate_HashRate(Year, Month, ETH earned this month):\n",
        "\n",
        "  estimated_hashrate_in_ghS = Production_Dictionary[(Year, Month) * ETH earned  this month)\n",
        "\n",
        "\n",
        "3. Use the Etherscan.io API to get all of the transactions from 5 large pools, and store that in pyspark on Google Colab. You will need to save that to someplace in the cloud so that you can get it later and put the public link. I think drop box is the right place for it.\n",
        "\n",
        " -> What I want the data to look like.\n",
        " This information is factual raw data. \n",
        "(Block Number of Transaction, Pool Address, Miner Address, Amount of ETH)\n",
        "\n",
        "\n",
        "Every Single Transaction will be mapped into this form. \n",
        "\n",
        "(Block Number, Day, Year, Month, Pool Address, Miner Address, Amount of ETH, Estimated_hashrate)\n",
        "\n",
        "This data will then be grouped by Miner_address and Month.\n",
        "\n",
        "Intermediate(after group by) result:\n",
        "\n",
        "Month, Year, Pool Address, Miner Address, ETH Earned this month, Estimated Hashrate, (ETH earned this month * Average Monthly Price of ETH) as monthly USD Revenue.\n",
        "\n",
        "Frame work for Miner Features\n",
        "This is how I am choosing to categorize Miners. \n",
        "\n",
        "## Miner Features\n",
        "Start month ( cast this as an int with January 2015 as 1). # month of first  income).\n",
        "End Month (cast this as an int with March 2021 as the 80 or whatever the number happens to be). Last month with income. or + infinity if that was march 2021. Age as number of full months,\n",
        "Median Hashrate, as size. \n",
        "\n",
        "\n",
        "\n",
        "Dummy Varibles for Pool address, \n",
        "Dummy Variables for if they went to a different\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Framework for OLS regression:\n",
        "\n",
        "##Independent variable: \n",
        "* Estimated Monthly Hashrate.\n",
        "\n",
        "###Dependent Variables\n",
        "Miner Features. \n",
        "Year\n",
        "Month,\n",
        "Average ETH price,\n",
        "average Gh/s value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eldsm3TLLL_l"
      },
      "source": [
        "### Setup Pyspark and Connect it to your google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoBD2liiLBZ0",
        "outputId": "758d0df4-4af1-4fc3-d5e3-c4f33a339414"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Open Connection to Google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "print(\"\\n\\n DRIVE SETUP\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n",
            "openjdk-8-jdk-headless is already the newest version (8u282-b08-0ubuntu1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "\n",
            " DRIVE SETUP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D57YpSnyLnN1"
      },
      "source": [
        "## Install Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HRjFYf1LmL-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import math\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from functools import reduce\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import DataType\n",
        "from pyspark import SparkContext, SparkConf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK8odYhULw-2"
      },
      "source": [
        "### Create Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1clEsoKVLwbN",
        "outputId": "65b5c774-c5cd-4404-f720-7642c6062229"
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\") # copy and pasted this, dont' know what it means\n",
        "# create the context\n",
        "# try catch block lets you start again from the top without causing problems. \n",
        "try:\n",
        "  sc.stop()\n",
        "  sc = pyspark.SparkContext(conf=conf)\n",
        "except:\n",
        "  sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "print('Successfully Created Spark Session')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully Created Spark Session\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgBL6_0bKH4H"
      },
      "source": [
        "## Methods to get transactions from Etherscan.io API\n",
        "\n",
        "\n",
        "### You will want some try catch logic to make sure that you are not needlessly pinging Etherscan.io"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7x-vmI5NPLL"
      },
      "source": [
        "def read_api_key():\n",
        "  api_key_json = open(r'/content/drive/MyDrive/etherScan_apiKey.json') # read in my private Etherscan.io API key from my Google Drive\n",
        "  etherscan_api_key =json.load(api_key_json)['key']\n",
        "  api_key_json.close()\n",
        "  return etherscan_api_key\n",
        "\n",
        "ETHERSCAN_API_KEY = read_api_key() # You api key is a constant so the variable name is UPPER CASE\n",
        "ethermine_wallet = '0xea674fdde714fd979de3edf0f56aa9716b898ec8'\n",
        "def query_normal_transactions(wallet_address, startblock=0, endblock=99999999): # returning a list here might be inefficient. \n",
        "  \"\"\"\n",
        "    Ping Etherscan.io and get the most recent 10k transactions for this wallet.\n",
        "    Each of these takes ~4 seconds\n",
        "  \"\"\"\n",
        "  api_request_text =f'https://api.etherscan.io/api?module=account&action=txlist&address={wallet_address}&startblock={startblock}&endblock={endblock}&sort=asc&apikey={ETHERSCAN_API_KEY}'\n",
        "     \n",
        "  response = requests.get(api_request_text) # Need to throttle this to 5 calls a second\n",
        "  api_response_text = json.loads(response.text) \n",
        "  # error handling to make sure that the call is going through\n",
        "  if api_response_text['message'] != 'OK': # used\n",
        "    print(f'you got an error at address={wallet_address} \\nstartblock= {startblock}\\nendblock={endblock}')\n",
        "\n",
        "  else:\n",
        "    transaction_list = api_response_text['result']\n",
        "    simplified_transactions = parse_normal_transactions(transaction_list=transaction_list, wallet_address=wallet_address)\n",
        "\n",
        "    #exclude the last N transactions since the might overlapp\n",
        "    largest_block = simplified_transactions[-1][2] # O(1)\n",
        "    while simplified_transactions[-1][2] == largest_block:\n",
        "      simplified_transactions.pop(-1) # O(1) * 100 upper bound.\n",
        "\n",
        "    column_names = ['to_address',\n",
        "                    'from_address',\n",
        "                    'block_number',\n",
        "                    'transaction_year',\n",
        "                    'transaction_month',\n",
        "                    'value_in_ether']\n",
        "    \n",
        "    # create a rdd of this batch of transactions\n",
        "    rdd = spark.createDataFrame(simplified_transactions, schema=column_names)\n",
        "    return rdd, largest_block\n",
        "\n",
        "def parse_normal_transactions(transaction_list: list, wallet_address):\n",
        "    \"\"\"\n",
        "      Converts transaction data into a easy to read from and excludes the last block\n",
        "    :param transaction_list: A list of dictionary objects. Each dictionary is a single transaction.\n",
        "    :return:\n",
        "        simple_transactions: a list of tuples storing (to_address, from_address, Block number, Value in Ether)\n",
        "                                      Data types are: (String, String, int, year,, float)\n",
        "    \"\"\"\n",
        "    simplified_transactions =[(trans_dict['to'],\n",
        "                              trans_dict['from'],\n",
        "                              int(trans_dict['blockNumber']),\n",
        "                              datetime.datetime.fromtimestamp(int(trans_dict['timeStamp'])).year,\n",
        "                               datetime.datetime.fromtimestamp(int(trans_dict['timeStamp'])).month,\n",
        "                              int(trans_dict['value'])/math.pow(10, 18))\n",
        "                              for trans_dict in transaction_list if trans_dict['from'] == wallet_address\n",
        "                              ]\n",
        "    return simplified_transactions"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEPpIh68GY-h"
      },
      "source": [
        "### Trying to make the code marginally faster. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ocR4dyogtJ1",
        "outputId": "ae0a4066-8d43-4ff0-8edd-ca99d30b4893"
      },
      "source": [
        "%%time\n",
        "ethermine_transaction_rdd, largest_block = query_normal_transactions(wallet_address=ethermine_wallet)\n",
        "for i in range(50):\n",
        "  next_chunk_rdd, largest_block = query_normal_transactions(wallet_address=ethermine_wallet, startblock=largest_block)\n",
        "  ethermine_transaction_rdd = ethermine_transaction_rdd.union(next_chunk_rdd)\n",
        "  print(f'call {i+1}') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "call 1\n",
            "call 2\n",
            "call 3\n",
            "call 4\n",
            "call 5\n",
            "call 6\n",
            "call 7\n",
            "call 8\n",
            "call 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZh48kDxZNUr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFj5VQVcbkKe"
      },
      "source": [
        "### Only save the group by month_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDla_iVrb7X2"
      },
      "source": [
        "rdd = ethermine_transaction_rdd\n",
        "rdd = rdd.withColumn('year_month', concat(rdd.transaction_year,lit(\"-\"),rdd.transaction_month)) # add the year_month column\n",
        "rdd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF3Oy0hIgbQ_"
      },
      "source": [
        "rdd.registerTempTable(\"ethermine_transactions\")\n",
        "\n",
        "monthly_miner_revenue_query =\"\"\"SELECT to_address, year_month, ROUND(sum(value_in_ether),9) as eth_earned\n",
        "FROM ethermine_transactions \n",
        "GROUP BY to_address, year_month\n",
        "ORDER BY to_address, year_month \n",
        "\"\"\"\n",
        "\n",
        "miners = spark.sql(monthly_miner_revenue_query)\n",
        "print(miners.count())\n",
        "miners = miners.withColumn('from_address', lit(ethermine_wallet))\n",
        "miners.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n85_oocFjsx-"
      },
      "source": [
        "## Save miner_grouped_rdd to a .csv file on your google drive. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Oq2b21jrfo"
      },
      "source": [
        "miners_df = miners.toPandas()\n",
        "miners_df.to_csv(r'/content/drive/MyDrive/miners.csv', index=False) # unclear what happens when there is alreadya file with stuff here.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReZSkiycoGoE"
      },
      "source": [
        "### Estimated Timecosts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcSH0GqqPo5X"
      },
      "source": [
        "# # How many API calls do you need to make?\n",
        "# n_transactions = 40000000\n",
        "# reccords_per_call = 10000\n",
        "# total_api_calls = n_transactions/reccords_per_call\n",
        "# base_time_cost_per_api_call = 5 #Loose Uppperbound\n",
        "# seconds_needed = (base_time_cost_per_api_call *total_api_calls) \n",
        "# hours = seconds_needed/3600\n",
        "# print(f'Upper bound for API calls for ethermine {total_api_calls_needed}')\n",
        "# print(f'Upper bound for hours to get all of Ethermine hours to get everything from ethermine {hours}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}