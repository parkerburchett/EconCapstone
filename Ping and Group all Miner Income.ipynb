{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook to Gather Ethermine Transaction data and do Cleaning and regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMhauPj7gGE6DMUFETq7SWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parkerburchett/EconCapstone/blob/main/Ping%20and%20Group%20all%20Miner%20Income.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-6c44YfUea1"
      },
      "source": [
        "# Constants\n",
        "parker_wallet = '0x76fb6d38f28c44a13380220df21363bd7af45ee1'\n",
        "ethan_wallet = '0xceb4d0ca821420cf2553b9e244f6b52364613f94'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H16oV8RENRNw"
      },
      "source": [
        "## This is the first draft of the notebook I am writing for my Economics Capstone to get a published paper. \\\n",
        "\n",
        "https://www.etherchain.org/charts Look at \"mining revenue\" gives USD value per day of a GH/s. Use this instead of building it yourself.\n",
        "### Outline: What will this notebook do?\n",
        "\n",
        "1. Use the Etherscan.io charts to convert into production \n",
        "* Block Number to Date.(Year, Month)\n",
        "* (Year, Month) to Monthly average Value of 1 day of mining at 1 GH/s (Etherscan.io chart). Just average by month. \n",
        "\n",
        "* (Year, Month) to Average ETH Price. \n",
        "\n",
        "\n",
        "* You will need a dictionary that looks like \n",
        "(Year, Month): Average Gh/s needed to mine make 1 ETH \n",
        "### Called production dictionary. \n",
        "\n",
        "2. You will need to write custom Estimate_hashrate \n",
        "\n",
        "*Pseudocode:\n",
        "  Estimate_HashRate(Year, Month, ETH earned this month):\n",
        "\n",
        "  estimated_hashrate_in_ghS = Production_Dictionary[(Year, Month) * ETH earned  this month)\n",
        "\n",
        "\n",
        "3. Use the Etherscan.io API to get all of the transactions from 5 large pools, and store that in pyspark on Google Colab. You will need to save that to someplace in the cloud so that you can get it later and put the public link. I think drop box is the right place for it.\n",
        "\n",
        " -> What I want the data to look like.\n",
        " This information is factual raw data. \n",
        "(Block Number of Transaction, Pool Address, Miner Address, Amount of ETH)\n",
        "\n",
        "\n",
        "Every Single Transaction will be mapped into this form. \n",
        "\n",
        "(Block Number, Day, Year, Month, Pool Address, Miner Address, Amount of ETH, Estimated_hashrate)\n",
        "\n",
        "This data will then be grouped by Miner_address and Month.\n",
        "\n",
        "Intermediate(after group by) result:\n",
        "\n",
        "Month, Year, Pool Address, Miner Address, ETH Earned this month, Estimated Hashrate, (ETH earned this month * Average Monthly Price of ETH) as monthly USD Revenue.\n",
        "\n",
        "Frame work for Miner Features\n",
        "This is how I am choosing to categorize Miners. \n",
        "\n",
        "## Miner Features\n",
        "Start month ( cast this as an int with January 2015 as 1). # month of first  income).\n",
        "End Month (cast this as an int with March 2021 as the 80 or whatever the number happens to be). Last month with income. or + infinity if that was march 2021. Age as number of full months,\n",
        "Median Hashrate, as size. \n",
        "\n",
        "\n",
        "\n",
        "Dummy Varibles for Pool address, \n",
        "Dummy Variables for if they went to a different\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Framework for OLS regression:\n",
        "\n",
        "##Independent variable: \n",
        "* Estimated Monthly Hashrate.\n",
        "\n",
        "###Dependent Variables\n",
        "Miner Features. \n",
        "Year\n",
        "Month,\n",
        "Average ETH price,\n",
        "average Gh/s value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eldsm3TLLL_l"
      },
      "source": [
        "### Setup Pyspark and Connect it to your google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoBD2liiLBZ0",
        "outputId": "16876df4-7d5f-43a6-a938-0be9e00dcba0"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Open Connection to Google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "print(\"\\n\\nDRIVE SETUP\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n",
            "openjdk-8-jdk-headless is already the newest version (8u282-b08-0ubuntu1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "\n",
            " DRIVE SETUP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D57YpSnyLnN1"
      },
      "source": [
        "## Install Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HRjFYf1LmL-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import math\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from functools import reduce\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import DataType\n",
        "from pyspark import SparkContext, SparkConf\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK8odYhULw-2"
      },
      "source": [
        "### Create Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1clEsoKVLwbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d72d40-f147-42e6-a344-bdc8c73bba2f"
      },
      "source": [
        "# create the spark session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\") # copy and pasted this, dont' know what it means\n",
        "# create the context\n",
        "# try catch block lets you start again from the top without causing problems. \n",
        "try:\n",
        "  sc.stop()\n",
        "  sc = pyspark.SparkContext(conf=conf)\n",
        "except:\n",
        "  sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "print('Successfully Created Spark Session')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully Created Spark Session\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gInzfyainX8H"
      },
      "source": [
        "## Establish constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7x-vmI5NPLL"
      },
      "source": [
        "def read_api_key():\n",
        "  api_key_json = open(r'/content/drive/MyDrive/etherScan_apiKey.json') # read in my private Etherscan.io API key from my Google Drive\n",
        "  etherscan_api_key =json.load(api_key_json)['key']\n",
        "  api_key_json.close()\n",
        "  return etherscan_api_key\n",
        "\n",
        "ETHERSCAN_API_KEY = read_api_key() # You api key is a constant so the variable name is UPPER CASE\n",
        "ethermine_wallet = '0xea674fdde714fd979de3edf0f56aa9716b898ec8'\n",
        "def query_normal_transactions(wallet_address, startblock=0, endblock=99999999): \n",
        "  \"\"\"\n",
        "    Ping Etherscan.io and get the most recent 10k transactions for this wallet.\n",
        "    Each of these takes ~4 seconds\n",
        "  \"\"\"\n",
        "  api_request_text =f'https://api.etherscan.io/api?module=account&action=txlist&address={wallet_address}&startblock={startblock}&endblock={endblock}&sort=asc&apikey={ETHERSCAN_API_KEY}'\n",
        "     \n",
        "  response = requests.get(api_request_text) # Need to throttle this to 5 calls a second\n",
        "  api_response_text = json.loads(response.text) \n",
        "  # error handling to make sure that the call is going through\n",
        "  if api_response_text['message'] != 'OK': # untested\n",
        "    print(f'you got an error at address={wallet_address} \\nstartblock= {startblock}\\nendblock={endblock}')\n",
        "\n",
        "  else:\n",
        "    transaction_list = api_response_text['result']\n",
        "    simplified_transactions = parse_normal_transactions(transaction_list=transaction_list, wallet_address=wallet_address)\n",
        "\n",
        "    #exclude the last N transactions since the might overlapp\n",
        "    largest_block = int(simplified_transactions[-1][2]) # O(1)\n",
        "    while simplified_transactions[-1][2] == largest_block:\n",
        "      simplified_transactions.pop(-1) # O(1) * 100 upper bound.\n",
        "\n",
        "    column_names = ['to_address',\n",
        "                    'from_address',\n",
        "                    'block_number',\n",
        "                    'transaction_year',\n",
        "                    'transaction_month',\n",
        "                    'value_in_ether']\n",
        "    \n",
        "    # create a rdd of this batch of transactions\n",
        "    rdd = spark.createDataFrame(simplified_transactions, schema=column_names)\n",
        "    return rdd, largest_block\n",
        "\n",
        "def parse_normal_transactions(transaction_list: list, wallet_address):\n",
        "    \"\"\"\n",
        "      Converts transaction data into a easy to read from and excludes the last block\n",
        "    :param transaction_list: A list of dictionary objects. Each dictionary is a single transaction.\n",
        "    :return:\n",
        "        simple_transactions: a list of tuples storing (to_address, from_address, Block number, Value in Ether)\n",
        "                                      Data types are: (String, String, int, year,, float)\n",
        "    \"\"\"\n",
        "    simplified_transactions =[(trans_dict['to'],\n",
        "                              trans_dict['from'],\n",
        "                              int(trans_dict['blockNumber']),\n",
        "                              datetime.datetime.fromtimestamp(int(trans_dict['timeStamp'])).year,\n",
        "                               datetime.datetime.fromtimestamp(int(trans_dict['timeStamp'])).month,\n",
        "                              int(trans_dict['value'])/math.pow(10, 18))\n",
        "                              for trans_dict in transaction_list if trans_dict['from'] == wallet_address\n",
        "                              ]\n",
        "    return simplified_transactions"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEPpIh68GY-h"
      },
      "source": [
        "## Next steps are to get a dictionary of block: time chunks. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYZqQ6IUlIMG",
        "outputId": "db832bbf-4c18-408e-f154-951373c2e429"
      },
      "source": [
        "start_year = 2016\n",
        "years = [datetime.datetime(start_year+i,1,1) for i in range(0,6)]\n",
        "unix_timestamp_years = [int(year.timestamp()) for year in years]\n",
        "unix_timestamp_march31_2016 = int(datetime.datetime(2021,3,31).timestamp())\n",
        "\n",
        "command_for_block_after_timestamp = f'https://api.etherscan.io/api?module=block&action=getblocknobytime&timestamp={unix_timestamp_years[0]}&closest=after&apikey={ETHERSCAN_API_KEY}'\n",
        "\n",
        "year_block_dict ={}\n",
        "\n",
        "for unix_time in unix_timestamp_years:\n",
        "  time.sleep(.2) # don't overwhelm the API\n",
        "  response = requests.get(f'https://api.etherscan.io/api?module=block&action=getblocknobytime&timestamp={unix_time}&closest=after&apikey={ETHERSCAN_API_KEY}')\n",
        "  response_text = json.loads(response.text) \n",
        "  block = response_text['result']\n",
        "  year_block_dict[int(datetime.datetime.fromtimestamp(unix_time).year)]= block\n",
        "\n",
        "print(year_block_dict)\n",
        "response = requests.get(f'https://api.etherscan.io/api?module=block&action=getblocknobytime&timestamp={unix_timestamp_march31_2016}&closest=after&apikey={ETHERSCAN_API_KEY}')\n",
        "response_text = json.loads(response.text) \n",
        "march_2021_block = response_text['result']\n",
        "march_2021_block\n",
        "year_block_dict['2022'] = march_2021_block # this is to make the code work. as written "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{2016: '778483', 2017: '2912407', 2018: '4832686', 2019: '6988615', 2020: '9193266', 2021: '11565019'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd57cH8m1g6z"
      },
      "source": [
        "def get_monthly_miner_income_statements_chunk(year,pool_address):\n",
        "  \"\"\"\n",
        "    Ping every transaction where {pool_address} is the sender and cast it as a pyspark dataframe.\n",
        "\n",
        "    add a year_month column. \n",
        "\n",
        "    Group by to_address and year_month\n",
        "\n",
        "    cast as a pandas df\n",
        "\n",
        "    Save the document to a file in this local runtime of google drive. \n",
        "  \"\"\"\n",
        "  start = datetime.datetime.now()\n",
        "  min_block = int(year_block_dict[year]) # if year =2018 then this gets all miner incomes in 2018\n",
        "  max_block = int(year_block_dict[year+1]) # all blocks must be within this range\n",
        "  i=0\n",
        "  transactions_rdd, largest_block = query_normal_transactions(wallet_address=pool_address, startblock=min_block, endblock=max_block)\n",
        "\n",
        "  while largest_block < max_block:\n",
        "    try:\n",
        "      # repeat the call picking up where you left off last call since you can only get 10,000 records per call\n",
        "      next_chunk_rdd, largest_block = query_normal_transactions(wallet_address=pool_address, startblock=largest_block) \n",
        "      transactions_rdd = transactions_rdd.union(next_chunk_rdd) # union removes duplicates\n",
        "      time_dif = datetime.datetime.now() - start\n",
        "      start = datetime.datetime.now()\n",
        "      # this lets you see progress\n",
        "      print(f'Call:{i} Time:{time_dif} Block:{largest_block} Blocks Left:{max_block - largest_block}')\n",
        "      i+=1\n",
        "    except:\n",
        "      print(f'you got an error at {largest_block}\\n {next_chunk_rdd.head(1)} ')\n",
        "\n",
        "  \n",
        "  # add the year_month column. \n",
        "  transactions_rdd = transactions_rdd.withColumn('year_month', concat(transactions_rdd.transaction_year,lit(\"-\"),\n",
        "                                                                      transactions_rdd.transaction_month))\n",
        "\n",
        "  transactions_rdd.registerTempTable(\"raw_transactions\") # I am pretty sure I don't need to deregister this. Unverified\n",
        "\n",
        "\n",
        "  monthly_miner_revenue_query =f\"\"\" SELECT to_address, year_month, ROUND(sum(value_in_ether),9) as eth_earned\n",
        "                                    FROM raw_transactions \n",
        "                                    Where block_number < {max_block} and block_number > {min_block}\n",
        "                                    GROUP BY to_address, year_month\n",
        "                                    ORDER BY to_address, year_month \n",
        "                                \"\"\"\n",
        "\n",
        "  miners_sql_df = spark.sql(monthly_miner_revenue_query)\n",
        "  miners_sql_df = miners_sql_df.withColumn('from_address', lit(pool_address)) # add the pool address back into the rdd\n",
        "\n",
        "  df = miners_sql_df.toPandas()\n",
        "  print('Verify that these months are correct')\n",
        "  print(df['year_month'].unique())\n",
        "  df.to_csv(f'/content/drive/MyDrive/transaction_data/miner_income_{year}_{pool_address}.csv',index=False) # save to your google drive\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01W1Nk_Y6Fh6",
        "outputId": "def90144-3d6f-4378-88f5-cd27fb2886bf"
      },
      "source": [
        "%%time\n",
        "\n",
        "# first test\n",
        "# get_monthly_miner_income_statements_chunk(2016,ethermine_wallet)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
            "Wall time: 8.82 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFY1_CDG9e5z"
      },
      "source": [
        "HIVEON_POOL= '0x1aD91ee08f21bE3dE0BA2ba6918E714dA6B45836'\n",
        "SPARK_POOL = '0x5A0b54D5dc17e0AadC383d2db43B0a0D3E029c4c'\n",
        "F2_POOL = '0x829BD824B016326A401d083B33D092293333A830'\n",
        "ZHIZHU_TOP_POOL = '0x04668Ec2f57cC15c381b461B9fEDaB5D451c8F7F'\n",
        "ETHERMINE_POOL = '0xEA674fdDe714fd979de3EdF0F56AA9716B898ec8'\n",
        "\n",
        "POOLS = [HIVEON_POOL, SPARK_POOL, F2_POOL, ZHIZHU_TOP_POOL, ETHERMINE_POOL]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD5W5Pnq8Gtm",
        "outputId": "c2f37bb7-47a9-4b59-cae6-57e2bdeb3dd4"
      },
      "source": [
        "%%time\n",
        "years = [2016,2017,2018,2019,2020,2021]\n",
        "for y in years:\n",
        "  get_monthly_miner_income_statements_chunk(y,ethermine_wallet)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Call:0 Time:0:00:14.548862 Block:1312289 Blocks Left:1600118\n",
            "Call:1 Time:0:00:04.069931 Block:1347595 Blocks Left:1564812\n",
            "Call:2 Time:0:00:05.801691 Block:1378032 Blocks Left:1534375\n",
            "Call:3 Time:0:00:03.584586 Block:1407645 Blocks Left:1504762\n",
            "Call:4 Time:0:00:04.458185 Block:1435260 Blocks Left:1477147\n",
            "Call:5 Time:0:00:04.125148 Block:1460803 Blocks Left:1451604\n",
            "Call:6 Time:0:00:04.368528 Block:1487553 Blocks Left:1424854\n",
            "Call:7 Time:0:00:03.859156 Block:1512372 Blocks Left:1400035\n",
            "Call:8 Time:0:00:03.782743 Block:1536853 Blocks Left:1375554\n",
            "Call:9 Time:0:00:04.036393 Block:1557198 Blocks Left:1355209\n",
            "Call:10 Time:0:00:03.818006 Block:1579298 Blocks Left:1333109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgBL6_0bKH4H"
      },
      "source": [
        "## Methods to get transactions from Etherscan.io API\n",
        "\n",
        "\n",
        "### You will want some try catch logic to make sure that you are not needlessly pinging Etherscan.io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxopgj-ejj_2"
      },
      "source": [
        "### Figure out what is the first and last block in each year. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ocR4dyogtJ1"
      },
      "source": [
        "# %%time\n",
        "# start = datetime.datetime.now()\n",
        "# ethermine_transaction_rdd, largest_block = query_normal_transactions(wallet_address=ethermine_wallet)\n",
        "# for i in range(4000): # the large call should be 4,000 ish. Just let it run while you are making dinner. see what happens.  \n",
        "#   try:\n",
        "#     next_chunk_rdd, largest_block = query_normal_transactions(wallet_address=ethermine_wallet, startblock=largest_block)\n",
        "#     ethermine_transaction_rdd = ethermine_transaction_rdd.union(next_chunk_rdd)\n",
        "#     if largest_block >= 12143978: # this the last block you want to consider\n",
        "#       break\n",
        "#     time_dif = datetime.datetime.now() -start\n",
        "#     start = datetime.datetime.now() \n",
        "#     print(f'Call:{i+1} Time:{time_dif}')\n",
        "#   except:\n",
        "#     print(f'you got an error at {largest_block}\\n {next_chunk_rdd.head(1)} ')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZh48kDxZNUr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFj5VQVcbkKe"
      },
      "source": [
        "### Only save the group by month_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDla_iVrb7X2"
      },
      "source": [
        "# rdd = ethermine_transaction_rdd\n",
        "# rdd = rdd.withColumn('year_month', concat(rdd.transaction_year,lit(\"-\"),rdd.transaction_month)) # add the year_month column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF3Oy0hIgbQ_"
      },
      "source": [
        "# rdd.registerTempTable(\"ethermine_transactions\")\n",
        "\n",
        "# monthly_miner_revenue_query =\"\"\"SELECT to_address, year_month, ROUND(sum(value_in_ether),9) as eth_earned\n",
        "# FROM ethermine_transactions \n",
        "# GROUP BY to_address, year_month\n",
        "# ORDER BY to_address, year_month \n",
        "# \"\"\"\n",
        "\n",
        "# miners = spark.sql(monthly_miner_revenue_query)\n",
        "# num_monthly_income_statements = miners.count()\n",
        "# print(f'You have {num_monthly_income_statements} income statements')\n",
        "# miners = miners.withColumn('from_address', lit(ethermine_wallet))\n",
        "# miners.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n85_oocFjsx-"
      },
      "source": [
        "## Save miner_grouped_rdd to a .csv file on your google drive. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Oq2b21jrfo"
      },
      "source": [
        "# miners_df = miners.toPandas()\n",
        "# miners_df.to_csv(r'/content/drive/MyDrive/miners.csv', index=False) # unclear what happens when there is alreadya file with stuff here.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAuBniUDvVEB"
      },
      "source": [
        "# with open(r'/content/drive/MyDrive/miners.csv', 'r') as data:\n",
        "#   num_lines_in_file = len(data.readlines())\n",
        "#   print(f'In your google drive you saved a file with: {num_lines_in_file} lines')\n",
        "#   print(f'There should be:                            {num_monthly_income_statements+1} lines ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReZSkiycoGoE"
      },
      "source": [
        "### Estimated Timecosts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcSH0GqqPo5X"
      },
      "source": [
        "# # How many API calls do you need to make?\n",
        "# n_transactions = 40000000\n",
        "# reccords_per_call = 10000\n",
        "# total_api_calls = n_transactions/reccords_per_call\n",
        "# base_time_cost_per_api_call = 5 #Loose Uppperbound\n",
        "# seconds_needed = (base_time_cost_per_api_call *total_api_calls) \n",
        "# hours = seconds_needed/3600\n",
        "# print(f'Upper bound for API calls for ethermine {total_api_calls_needed}') # upper bound is 5 hours.\n",
        "# print(f'Upper bound for hours to get all of Ethermine hours to get everything from ethermine {hours}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}